<class 'numpy.ndarray'>
(876, 10, 16, 16)
(876, 24, 16, 16)
Number of training samples:  876
Number of testing samples:  876
Dimension of Data: (876, 10, 16, 16) (876, 10, 16, 16)
Number of all Data 1752
Number of batches:  55
  0%|          | 0/2000 [00:00<?, ?it/s]Unet_training_torch.py:388: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  inputs = Variable(inputs, volatile=True).cuda()
Unet_training_torch.py:389: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  labels = Variable(labels, volatile=True).cuda()
libibverbs: Warning: couldn't open config directory '/etc/libibverbs.d'.
                                          0%|          | 0/2000 [00:06<?, ?it/s][1,     5] loss: 0.001 
                                        [1,    10] loss: -0.001 
  0%|          | 0/2000 [00:07<?, ?it/s]                                        [1,    15] loss: 0.000 
  0%|          | 0/2000 [00:08<?, ?it/s]                                        [1,    20] loss: -0.001 
  0%|          | 0/2000 [00:09<?, ?it/s]                                        [1,    25] loss: 0.000 
  0%|          | 0/2000 [00:10<?, ?it/s]                                        [1,    30] loss: 0.000 
  0%|          | 0/2000 [00:11<?, ?it/s]                                        [1,    35] loss: -0.001 
  0%|          | 0/2000 [00:12<?, ?it/s]                                        [1,    40] loss: 0.000 
  0%|          | 0/2000 [00:13<?, ?it/s][1,    45] loss: 0.000 
                                          0%|          | 0/2000 [00:14<?, ?it/s]                                        [1,    50] loss: 0.001 
  0%|          | 0/2000 [00:15<?, ?it/s]                                        [1,    55] loss: -0.000 
  0%|          | 0/2000 [00:15<?, ?it/s]Pruning threshold: 1.2034777796543494e-07
layer index: 2 	 total params: 23520 	 remaining params: 23520
layer index: 8 	 total params: 115200 	 remaining params: 115199
Total conv params: 138720, Pruned conv params: 1.0, Pruned ratio: 7.2087659646058455e-06
Pruning threshold: 1.0181833848266209e-10
layer index: 14 	 total params: 1474560 	 remaining params: 1474560
layer index: 18 	 total params: 614400 	 remaining params: 614400
layer index: 22 	 total params: 40960 	 remaining params: 40960
layer index: 25 	 total params: 40960 	 remaining params: 40960
layer index: 29 	 total params: 614400 	 remaining params: 614400
layer index: 33 	 total params: 1474560 	 remaining params: 1474560
layer index: 49 	 total params: 37748736 	 remaining params: 37748736
Total Linear params: 42008576, Pruned Linear params: 0.0, Pruned ratio: 0.0
Pruning threshold: 2.7306636951607288e-08
layer index: 38 	 total params: 115200 	 remaining params: 115200
layer index: 44 	 total params: 56448 	 remaining params: 56447
Total ConvTranspose2d params: 171648, Pruned ConvTranspose2d params: 1.0, Pruned ratio: 5.825876087328652e-06
Unet_training_torch.py:516: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  tinputs = Variable(tinputs, volatile=True).cuda()
Unet_training_torch.py:517: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  tlabels = Variable(tlabels, volatile=True).cuda()
  0%|          | 1/2000 [00:28<15:51:09, 28.55s/it]test loss:  -8.616005569902799e-05
                                                   [2,     5] loss: -0.001   0%|          | 1/2000 [00:29<15:51:09, 28.55s/it]
                                                   [2,    10] loss: 0.000 
  0%|          | 1/2000 [00:30<15:51:09, 28.55s/it]                                                   [2,    15] loss: -0.001 
  0%|          | 1/2000 [00:31<15:51:09, 28.55s/it][2,    20] loss: 0.001 
                                                     0%|          | 1/2000 [00:32<15:51:09, 28.55s/it]                                                     0%|          | 1/2000 [00:33<15:51:09, 28.55s/it][2,    25] loss: 0.001 
                                                   [2,    30] loss: 0.000 
  0%|          | 1/2000 [00:34<15:51:09, 28.55s/it]                                                   [2,    35] loss: -0.002 
  0%|          | 1/2000 [00:35<15:51:09, 28.55s/it][2,    40] loss: -0.000 
                                                     0%|          | 1/2000 [00:36<15:51:09, 28.55s/it]                                                   [2,    45] loss: -0.000 
  0%|          | 1/2000 [00:37<15:51:09, 28.55s/it]                                                   [2,    50] loss: 0.000 
  0%|          | 1/2000 [00:38<15:51:09, 28.55s/it]                                                     0%|          | 1/2000 [00:39<15:51:09, 28.55s/it][2,    55] loss: -0.000 
Pruning threshold: 6.767739932911354e-07
layer index: 2 	 total params: 23520 	 remaining params: 23520
layer index: 8 	 total params: 115200 	 remaining params: 115199
Total conv params: 138720, Pruned conv params: 1.0, Pruned ratio: 7.2087659646058455e-06
Pruning threshold: 1.3910769237901466e-12
layer index: 14 	 total params: 1474560 	 remaining params: 1474560
layer index: 18 	 total params: 614400 	 remaining params: 614400
layer index: 22 	 total params: 40960 	 remaining params: 40960
layer index: 25 	 total params: 40960 	 remaining params: 40960
layer index: 29 	 total params: 614400 	 remaining params: 614400
layer index: 33 	 total params: 1474560 	 remaining params: 1474560
layer index: 49 	 total params: 37748736 	 remaining params: 37748736
Total Linear params: 42008576, Pruned Linear params: 0.0, Pruned ratio: 0.0
Pruning threshold: 2.9605498497176086e-08
layer index: 38 	 total params: 115200 	 remaining params: 115199
layer index: 44 	 total params: 56448 	 remaining params: 56448
Total ConvTranspose2d params: 171648, Pruned ConvTranspose2d params: 1.0, Pruned ratio: 5.825876087328652e-06
test loss:  5.62274213404056e-05
  0%|          | 2/2000 [00:51<14:55:43, 26.90s/it][3,     5] loss: 0.000 
                                                     0%|          | 2/2000 [00:52<14:55:43, 26.90s/it][3,    10] loss: 0.001 
                                                     0%|          | 2/2000 [00:53<14:55:43, 26.90s/it]                                                     0%|          | 2/2000 [00:54<14:55:43, 26.90s/it][3,    15] loss: -0.001 
                                                     0%|          | 2/2000 [00:55<14:55:43, 26.90s/it][3,    20] loss: -0.000 
[3,    25] loss: 0.001 
                                                     0%|          | 2/2000 [00:56<14:55:43, 26.90s/it]                                                     0%|          | 2/2000 [00:57<14:55:43, 26.90s/it][3,    30] loss: 0.001 
[3,    35] loss: -0.001 
                                                     0%|          | 2/2000 [00:58<14:55:43, 26.90s/it]  0%|          | 2/2000 [00:58<16:13:17, 29.23s/it]
Traceback (most recent call last):
  File "Unet_training_torch.py", line 394, in <module>
    outputs = net(inputs,num_bits=bit_width)
  File "/home/yz87/anaconda3/envs/torchpy3_7/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/yz87/anaconda3/envs/torchpy3_7/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 154, in forward
    replicas = self.replicate(self.module, self.device_ids[:len(inputs)])
  File "/home/yz87/anaconda3/envs/torchpy3_7/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 159, in replicate
    return replicate(module, device_ids, not torch.is_grad_enabled())
  File "/home/yz87/anaconda3/envs/torchpy3_7/lib/python3.7/site-packages/torch/nn/parallel/replicate.py", line 131, in replicate
    for j in range(num_replicas):
KeyboardInterrupt
